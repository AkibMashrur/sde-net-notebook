{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "301120_sde_regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AzxTU7XB_DX",
        "outputId": "c56a9f83-bd33-4705-a065-cfe2d94577d9"
      },
      "source": [
        "# mount\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULeGGkJ-DuC6"
      },
      "source": [
        "## dataloader\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, normalize\n",
        "from sklearn import datasets\n",
        "def load_MSD():\n",
        "# Load the raw data.\n",
        "    num_attributes = 90\n",
        "    names = ['Year'] + ['Attr_{}'.format(i) for i in range(num_attributes)]\n",
        "    df = pd.read_csv('/content/drive/My Drive/Research/Data/YearPredictionMSD.txt', header=None, names=names)\n",
        "\n",
        "# Validate the data.\n",
        "    num_examples = 515345\n",
        "    assert len(df.columns) == num_attributes + 1\n",
        "    assert len(df) == num_examples\n",
        "    assert not df.isnull().values.any()\n",
        "\n",
        "\n",
        "# Train/test split. See \"Data Set Information\".\n",
        "    num_train = 463715\n",
        "    df = df.values\n",
        "    train = df[:num_train]\n",
        "    test = df[num_train:]\n",
        "    del df\n",
        "\n",
        "\n",
        "# Seperate inputs and outputs.\n",
        "    X_train, y_train = train[:, 1:], train[:, 0]\n",
        "    X_test, y_test = test[:, 1:], test[:, 0]\n",
        "    del train\n",
        "    del test\n",
        "    \n",
        "    standardize = StandardScaler().fit(X_train)\n",
        "    X_train = standardize.transform(X_train)\n",
        "    X_test = standardize.transform(X_test)\n",
        "\n",
        "    y_train1 = np.expand_dims(y_train, axis=1)\n",
        "    y_test1 = np.expand_dims(y_test , axis=1)\n",
        "\n",
        "    standardize2 = StandardScaler().fit(y_train1)\n",
        "    y_train = standardize2.transform(y_train1)\n",
        "    y_train = np.squeeze(y_train)\n",
        "\n",
        "    y_test1 = np.expand_dims(y_test, axis=1)\n",
        "    y_test = standardize2.transform(y_test1)\n",
        "    y_test = np.squeeze(y_test)\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "def load_boston():\n",
        "    boston = datasets.load_boston()\n",
        "    x = boston.data\n",
        "    X = np.concatenate((x,x), axis=1)\n",
        "    for i in range(5):\n",
        "        X = np.concatenate((X,x), axis=1)\n",
        "    X = X[:,0:-1]\n",
        "    standardize = StandardScaler().fit(X)\n",
        "    X = standardize.transform(X)\n",
        "    return X\n",
        "\n",
        "\n",
        "\n",
        "def load_dataset(dataset):\n",
        "    if dataset == 'MSD':\n",
        "        X_train, y_train, X_test, y_test = load_MSD()\n",
        "        return X_train, y_train, X_test, y_test\n",
        "    if dataset == 'boston':\n",
        "        x = load_boston()\n",
        "        return x\n",
        "    \n",
        "    \n",
        "x = load_dataset('boston')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ2VqJHaDTOu",
        "outputId": "acc1f1fa-a18c-4c54-e905-1d1984de872f"
      },
      "source": [
        "## model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import torch.nn.init as init\n",
        "import math\n",
        "\n",
        "\n",
        "__all__ = ['SDENet']\n",
        "\n",
        "class Drift(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Drift, self).__init__()\n",
        "        self.fc = nn.Linear(50, 50)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "    def forward(self, t, x):\n",
        "        out = self.relu(self.fc(x))\n",
        "        return out    \n",
        "\n",
        "\n",
        "\n",
        "class Diffusion(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Diffusion, self).__init__()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc1 = nn.Linear(50, 100)\n",
        "        self.fc2 = nn.Linear(100, 1)\n",
        "    def forward(self, t, x):\n",
        "        out = self.relu(self.fc1(x))\n",
        "        out = self.fc2(out)\n",
        "        out = torch.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "    \n",
        "class SDENet(nn.Module):\n",
        "    def __init__(self, layer_depth):\n",
        "        super(SDENet, self).__init__()\n",
        "        self.layer_depth = layer_depth\n",
        "        self.downsampling_layers = nn.Linear(90, 50)\n",
        "        self.drift = Drift()\n",
        "        self.diffusion = Diffusion()\n",
        "        self.fc_layers = nn.Sequential(nn.ReLU(inplace=True), nn.Linear(50, 2))\n",
        "        self.deltat = 4./self.layer_depth\n",
        "        self.sigma = 0.5\n",
        "    def forward(self, x, training_diffusion=False):\n",
        "        out = self.downsampling_layers(x)\n",
        "        if not training_diffusion:\n",
        "            t = 0\n",
        "            diffusion_term = self.sigma*self.diffusion(t, out)\n",
        "            for i in range(self.layer_depth):\n",
        "                t = 4*(float(i))/self.layer_depth\n",
        "                out = out + self.drift(t, out)*self.deltat + diffusion_term*math.sqrt(self.deltat)*torch.randn_like(out).to(x)\n",
        "\n",
        "            final_out = self.fc_layers(out) \n",
        "            mean = final_out[:,0]\n",
        "            sigma = F.softplus(final_out[:,1])+1e-3\n",
        "            return mean, sigma\n",
        "            \n",
        "        else:\n",
        "            t = 0\n",
        "            final_out = self.diffusion(t, out.detach())  \n",
        "            return final_out\n",
        "\n",
        "def test():\n",
        "    model = SDENet(layer_depth=6)\n",
        "    return model  \n",
        " \n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = test()\n",
        "    num_params = count_parameters(model)\n",
        "    print(num_params)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ2h80v5Day7"
      },
      "source": [
        "#  metrics\n",
        "\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import time\n",
        "from scipy import misc\n",
        "\n",
        "def tpr95(dir_name, task = 'OOD'):\n",
        "    #calculate the falsepositive error when tpr is 95%\n",
        "    if task == 'OOD':\n",
        "        cifar = np.loadtxt('%s/confidence_Base_In.txt'%dir_name, delimiter=',')\n",
        "        other = np.loadtxt('%s/confidence_Base_Out.txt'%dir_name, delimiter=',')\n",
        "    elif task == 'mis':\n",
        "        cifar = np.loadtxt('%s/confidence_Base_Succ.txt'%dir_name, delimiter=',')\n",
        "        other = np.loadtxt('%s/confidence_Base_Err.txt'%dir_name, delimiter=',')\n",
        "\n",
        "\n",
        "    Y1 = other\n",
        "    X1 = cifar\n",
        "    end = np.max([np.max(X1), np.max(Y1)])\n",
        "    start = np.min([np.min(X1),np.min(Y1)])\n",
        "    gap = (end- start)/200000 # precision:200000\n",
        "\n",
        "    total = 0.0\n",
        "    fpr = 0.0\n",
        "    for delta in np.arange(start, end, gap):\n",
        "        tpr = np.sum(np.sum(X1 >= delta)) / np.float(len(X1))\n",
        "        error2 = np.sum(np.sum(Y1 > delta)) / np.float(len(Y1))\n",
        "        if tpr <= 0.96 and tpr >= 0.94:\n",
        "            fpr += error2\n",
        "            total += 1\n",
        "    if total == 0:\n",
        "        print('corner case')\n",
        "        fprBase = 1\n",
        "    else:\n",
        "        fprBase = fpr/total\n",
        "\n",
        "    return fprBase\n",
        "\n",
        "\n",
        "def auroc(dir_name, task = 'OOD'):\n",
        "    #calculate the AUROC\n",
        "    if task == 'OOD':\n",
        "        f1 = open('%s/Update_Base_ROC_tpr.txt'%dir_name, 'w')\n",
        "        f2 = open('%s/Update_Base_ROC_fpr.txt'%dir_name, 'w')\n",
        "        cifar = np.loadtxt('%s/confidence_Base_In.txt'%dir_name, delimiter=',')\n",
        "        other = np.loadtxt('%s/confidence_Base_Out.txt'%dir_name, delimiter=',')\n",
        "    elif task == 'mis':\n",
        "        f1 = open('%s/Update_Base_ROC_tpr_mis.txt'%dir_name, 'w')\n",
        "        f2 = open('%s/Update_Base_ROC_fpr_mis.txt'%dir_name, 'w')\n",
        "        cifar = np.loadtxt('%s/confidence_Base_Succ.txt'%dir_name, delimiter=',')\n",
        "        other = np.loadtxt('%s/confidence_Base_Err.txt'%dir_name, delimiter=',')\n",
        "\n",
        "\n",
        "    Y1 = other\n",
        "    X1 = cifar\n",
        "    end = np.max([np.max(X1), np.max(Y1)])\n",
        "    start = np.min([np.min(X1),np.min(Y1)])\n",
        "    gap = (end- start)/200000\n",
        "\n",
        "    aurocBase = 0.0\n",
        "    fprTemp = 1.0\n",
        "    for delta in np.arange(start, end, gap):\n",
        "        tpr = np.sum(np.sum(X1 >= delta)) / np.float(len(X1))\n",
        "        fpr = np.sum(np.sum(Y1 > delta)) / np.float(len(Y1))\n",
        "        f1.write(\"{}\\n\".format(tpr))\n",
        "        f2.write(\"{}\\n\".format(fpr))\n",
        "        aurocBase += (-fpr+fprTemp)*tpr\n",
        "        fprTemp = fpr\n",
        "    f1.close()\n",
        "    f2.close()\n",
        "    return aurocBase\n",
        "\n",
        "def auprIn(dir_name, task ='OOD'):\n",
        "    #calculate the AUPR\n",
        "    if task == 'OOD':\n",
        "        cifar = np.loadtxt('%s/confidence_Base_In.txt'%dir_name, delimiter=',')\n",
        "        other = np.loadtxt('%s/confidence_Base_Out.txt'%dir_name, delimiter=',')\n",
        "    elif task == 'mis':\n",
        "        cifar = np.loadtxt('%s/confidence_Base_Succ.txt'%dir_name, delimiter=',')\n",
        "        other = np.loadtxt('%s/confidence_Base_Err.txt'%dir_name, delimiter=',')\n",
        "\n",
        "    precisionVec = []\n",
        "    recallVec = []\n",
        "    Y1 = other\n",
        "    X1 = cifar\n",
        "    end = np.max([np.max(X1), np.max(Y1)])\n",
        "    start = np.min([np.min(X1),np.min(Y1)])\n",
        "    gap = (end- start)/200000\n",
        "\n",
        "    auprBase = 0.0\n",
        "    recallTemp = 1.0\n",
        "    for delta in np.arange(start, end, gap):\n",
        "        tp = np.sum(np.sum(X1 >= delta)) #/ np.float(len(X1))\n",
        "        fp = np.sum(np.sum(Y1 >= delta)) #/ np.float(len(Y1))\n",
        "        if tp + fp == 0: continue\n",
        "        precision = tp / (tp + fp)\n",
        "        recall = tp/ np.float(len(X1))\n",
        "        precisionVec.append(precision)\n",
        "        recallVec.append(recall)\n",
        "        auprBase += (recallTemp-recall)*precision\n",
        "        recallTemp = recall\n",
        "    auprBase += recall * precision\n",
        "\n",
        "    return auprBase\n",
        "\n",
        "def auprOut(dir_name, task = 'OOD'):\n",
        "    #calculate the AUPR\n",
        "    if task == 'OOD':\n",
        "        cifar = np.loadtxt('%s/confidence_Base_In.txt'%dir_name, delimiter=',')\n",
        "        other = np.loadtxt('%s/confidence_Base_Out.txt'%dir_name, delimiter=',')\n",
        "    elif task == 'mis':\n",
        "        cifar = np.loadtxt('%s/confidence_Base_Succ.txt'%dir_name, delimiter=',')\n",
        "        other = np.loadtxt('%s/confidence_Base_Err.txt'%dir_name, delimiter=',')\n",
        "    Y1 = other\n",
        "    X1 = cifar\n",
        "    end = np.max([np.max(X1), np.max(Y1)])\n",
        "    start = np.min([np.min(X1),np.min(Y1)])\n",
        "    gap = (end- start)/200000\n",
        "\n",
        "    auprBase = 0.0\n",
        "    recallTemp = 1.0\n",
        "    for delta in np.arange(end, start, -gap):\n",
        "        fp = np.sum(np.sum(X1 < delta)) #/ np.float(len(X1))\n",
        "        tp = np.sum(np.sum(Y1 < delta)) #/ np.float(len(Y1))\n",
        "        if tp + fp == 0: break\n",
        "        precision = tp / (tp+fp)\n",
        "        recall = tp/np.float(len(Y1))\n",
        "        auprBase += (recallTemp-recall)*precision\n",
        "        recallTemp = recall\n",
        "    auprBase += recall * precision\n",
        "\n",
        "    return auprBase\n",
        "\n",
        "def detection(dir_name, task = 'OOD'):\n",
        "    #calculate the minimum detection error\n",
        "    if task == 'OOD':\n",
        "        cifar = np.loadtxt('%s/confidence_Base_In.txt'%dir_name, delimiter=',')\n",
        "        other = np.loadtxt('%s/confidence_Base_Out.txt'%dir_name, delimiter=',')\n",
        "    elif task == 'mis':\n",
        "        cifar = np.loadtxt('%s/confidence_Base_Succ.txt'%dir_name, delimiter=',')\n",
        "        other = np.loadtxt('%s/confidence_Base_Err.txt'%dir_name, delimiter=',')\n",
        "\n",
        "    Y1 = other\n",
        "    X1 = cifar\n",
        "    end = np.max([np.max(X1), np.max(Y1)])\n",
        "    start = np.min([np.min(X1),np.min(Y1)])\n",
        "    gap = (end- start)/200000\n",
        "\n",
        "    errorBase = 1.0\n",
        "    for delta in np.arange(start, end, gap):\n",
        "        tpr = np.sum(np.sum(X1 < delta)) / np.float(len(X1))\n",
        "        error2 = np.sum(np.sum(Y1 > delta)) / np.float(len(Y1))\n",
        "        errorBase = np.minimum(errorBase, (tpr+error2)/2.0)\n",
        "\n",
        "    return errorBase\n",
        "\n",
        "def metric(dir_name, task):\n",
        "    print(\"{}{:>34}\".format(task, \"Performance of Baseline detector\"))\n",
        "    fprBase = tpr95(dir_name, task)\n",
        "    print(\"{:20}{:13.3f}%\".format(\"TNR at TPR 95%:\", (1-fprBase)*100))\n",
        "    aurocBase = auroc(dir_name, task)\n",
        "    print(\"{:20}{:13.3f}%\".format(\"AUROC:\",aurocBase*100))\n",
        "    errorBase = detection(dir_name,task)\n",
        "    print(\"{:20}{:13.3f}%\".format(\"Detection acc:\",(1-errorBase)*100))\n",
        "    auprinBase = auprIn(dir_name,task)\n",
        "    print(\"{:20}{:13.3f}%\".format(\"AUPR In:\",auprinBase*100))\n",
        "    auproutBase = auprOut(dir_name,task)\n",
        "    print(\"{:20}{:13.3f}%\".format(\"AUPR Out:\",auproutBase*100))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52_B3wp9DctG",
        "outputId": "762e647f-1d2d-4d83-8f03-745c4b010b1f"
      },
      "source": [
        "# training\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "# import data_loader\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "# import models \n",
        "\n",
        "# parser = argparse.ArgumentParser(description='PyTorch SDENet Training')\n",
        "# parser.add_argument('--epochs', type=int, default=60, help='number of epochs to train')\n",
        "# parser.add_argument('--lr', default=1e-4, type=float, help='learning rate')\n",
        "# parser.add_argument('--lr2', default=0.01, type=float, help='learning rate')\n",
        "# parser.add_argument('--gpu', type=int, default=0)\n",
        "# parser.add_argument('--seed', type=float, default=0)\n",
        "# parser.add_argument('--droprate', type=float, default=0.1, help='learning rate decay')\n",
        "# parser.add_argument('--decreasing_lr', default=[20], nargs='+', help='decreasing strategy')\n",
        "# parser.add_argument('--decreasing_lr2', default=[], nargs='+', help='decreasing strategy')\n",
        "\n",
        "# args = parser.parse_args()\n",
        "# print(args)\n",
        "\n",
        "class args():\n",
        "  gpu = 1\n",
        "  seed = 0\n",
        "  lr = 1e-4\n",
        "  lr2 = 0.01\n",
        "  epochs = 50\n",
        "  droprate = 0.1\n",
        "  decreasing_lr = [20]\n",
        "  decreasing_lr2 = []\n",
        "\n",
        "# args.gpu = 1\n",
        "\n",
        "# device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n",
        "device = 'cuda'\n",
        "\n",
        "batch_size = 128\n",
        "Iter = 3622\n",
        "Iter_test = 403\n",
        "target_scale = 10.939756\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "if device == 'cuda':\n",
        "    cudnn.benchmark = True\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "\n",
        "X_train, y_train, X_test, y_test = load_dataset('MSD')\n",
        "\n",
        "X_train = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
        "X_test = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
        "y_train = torch.from_numpy(y_train).type(torch.FloatTensor)\n",
        "y_test = torch.from_numpy(y_test).type(torch.FloatTensor)\n",
        "\n",
        "# Model\n",
        "print('==> Building model..')\n",
        "net = SDENet(4)\n",
        "net = net.to(device)\n",
        "# net = net.to('cuda')\n",
        "\n",
        "\n",
        "real_label = 0\n",
        "fake_label = 1\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimizer_F = optim.SGD([ {'params': net.downsampling_layers.parameters()}, {'params': net.drift.parameters()},\n",
        "{'params': net.fc_layers.parameters()}], lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "optimizer_G = optim.SGD([ {'params': net.diffusion.parameters()}], lr=args.lr2, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "def nll_loss(y, mean, sigma):\n",
        "    loss = torch.mean(torch.log(sigma**2)+(y-mean)**2/(sigma**2))\n",
        "    return loss\n",
        "def mse(y, mean):\n",
        "    loss = torch.mean((y-mean)**2)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def load_training(iternum):\n",
        "    x = X_train[iternum*batch_size:(iternum+1)*batch_size]\n",
        "    y = y_train[iternum*batch_size:(iternum+1)*batch_size]\n",
        "    return x, y\n",
        "\n",
        "def load_test(iternum):\n",
        "    x = X_test[iternum*batch_size:(iternum+1)*batch_size]\n",
        "    y = y_test[iternum*batch_size:(iternum+1)*batch_size]\n",
        "    return x, y\n",
        "\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    if epoch == 0:\n",
        "        net.sigma = 0.1\n",
        "    if epoch == 30:\n",
        "        net.sigma = 0.5\n",
        "    train_loss = 0\n",
        "    train_loss_in = 0\n",
        "    train_loss_out = 0\n",
        "    total = 0\n",
        "    for iternum in range(Iter):\n",
        "        inputs, targets = load_training(iternum)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer_F.zero_grad()\n",
        "        mean, sigma = net(inputs)\n",
        "        loss = nll_loss(targets, mean, sigma)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), 100.)\n",
        "        optimizer_F.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        label = torch.full((batch_size,1), real_label, device=device).to(torch.float32)\n",
        "        optimizer_G.zero_grad()\n",
        "        predict_in = net(inputs, training_diffusion=True).to(torch.float32)\n",
        "        loss_in = criterion(predict_in, label)\n",
        "        loss_in.backward()\n",
        "        label.fill_(fake_label)\n",
        "\n",
        "        inputs_out = 2*torch.randn(batch_size, 90, device = device)+inputs\n",
        "        predict_out = net(inputs_out, training_diffusion=True)\n",
        "        loss_out = criterion(predict_out, label)\n",
        "        \n",
        "        loss_out.backward()\n",
        "        train_loss_out += loss_out.item()\n",
        "        train_loss_in += loss_in.item()\n",
        "        optimizer_G.step()\n",
        "      \n",
        "    print('Train epoch:{} \\tLoss: {:.6f}| Loss_in: {:.6f}| Loss_out: {:.6f}'.format(epoch, train_loss/Iter, train_loss_in/Iter, train_loss_out/Iter))\n",
        "\n",
        "def test(epoch):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for iternum in range(Iter_test):\n",
        "            inputs, targets = load_test(iternum)\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            current_mean = 0\n",
        "            for i in range(10):\n",
        "                mean, sigma = net(inputs)\n",
        "                current_mean = current_mean + mean\n",
        "            current_mean = current_mean/10\n",
        "            loss = mse(targets, current_mean)*target_scale\n",
        "            test_loss += loss.item()\n",
        "    \n",
        "    print('Test epoch:{} \\tLoss: {:.6f}'.format(epoch, np.sqrt(test_loss/Iter_test)))\n",
        "           \n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(0, args.epochs):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    if epoch in args.decreasing_lr:\n",
        "        for param_group in optimizer_F.param_groups:\n",
        "            param_group['lr'] *= args.droprate\n",
        "\n",
        "    if epoch in args.decreasing_lr2:\n",
        "        for param_group in optimizer_G.param_groups:\n",
        "            param_group['lr'] *= args.droprate\n",
        "\n",
        "\n",
        "if not os.path.isdir('./save_sdenet_msd'):\n",
        "    os.makedirs('./save_sdenet_msd')\n",
        "torch.save(net.state_dict(),'./save_sdenet_msd/final_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "Train epoch:0 \tLoss: 0.538097| Loss_in: 0.101449| Loss_out: 0.053645\n",
            "Test epoch:0 \tLoss: 2.848383\n",
            "\n",
            "Epoch: 1\n",
            "Train epoch:1 \tLoss: 0.332331| Loss_in: 0.051145| Loss_out: 0.026649\n",
            "Test epoch:1 \tLoss: 2.804849\n",
            "\n",
            "Epoch: 2\n",
            "Train epoch:2 \tLoss: 0.283098| Loss_in: 0.044123| Loss_out: 0.023556\n",
            "Test epoch:2 \tLoss: 2.778974\n",
            "\n",
            "Epoch: 3\n",
            "Train epoch:3 \tLoss: 0.252933| Loss_in: 0.039622| Loss_out: 0.021917\n",
            "Test epoch:3 \tLoss: 2.759232\n",
            "\n",
            "Epoch: 4\n",
            "Train epoch:4 \tLoss: 0.231624| Loss_in: 0.038079| Loss_out: 0.021433\n",
            "Test epoch:4 \tLoss: 2.743779\n",
            "\n",
            "Epoch: 5\n",
            "Train epoch:5 \tLoss: 0.214659| Loss_in: 0.036348| Loss_out: 0.020486\n",
            "Test epoch:5 \tLoss: 2.733092\n",
            "\n",
            "Epoch: 6\n",
            "Train epoch:6 \tLoss: 0.200562| Loss_in: 0.035378| Loss_out: 0.020066\n",
            "Test epoch:6 \tLoss: 2.725365\n",
            "\n",
            "Epoch: 7\n",
            "Train epoch:7 \tLoss: 0.189228| Loss_in: 0.034552| Loss_out: 0.019780\n",
            "Test epoch:7 \tLoss: 2.720180\n",
            "\n",
            "Epoch: 8\n",
            "Train epoch:8 \tLoss: 0.179095| Loss_in: 0.033464| Loss_out: 0.019073\n",
            "Test epoch:8 \tLoss: 2.716233\n",
            "\n",
            "Epoch: 9\n",
            "Train epoch:9 \tLoss: 0.170488| Loss_in: 0.032605| Loss_out: 0.018779\n",
            "Test epoch:9 \tLoss: 2.712325\n",
            "\n",
            "Epoch: 10\n",
            "Train epoch:10 \tLoss: 0.162705| Loss_in: 0.032073| Loss_out: 0.018576\n",
            "Test epoch:10 \tLoss: 2.709473\n",
            "\n",
            "Epoch: 11\n",
            "Train epoch:11 \tLoss: 0.155490| Loss_in: 0.031355| Loss_out: 0.018361\n",
            "Test epoch:11 \tLoss: 2.707238\n",
            "\n",
            "Epoch: 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVJW7HHoDgBp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}